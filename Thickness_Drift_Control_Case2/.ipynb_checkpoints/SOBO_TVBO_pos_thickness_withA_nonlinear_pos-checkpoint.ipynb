{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botorch\n",
    "import torch\n",
    "import matlab.engine\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from scipy.io import savemat\n",
    "\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.cd(r'./MPC_BO_Thickness_ver5.0_CASE2_nonlinear_Varying_Thickness_Temp_s', nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the second return is 1 if succeed, 0 otherwise\n",
    "def problem_wrapper(dA=1.5):\n",
    "    def problem(Tref, Iref, Aref1, Aref2, Aref3):\n",
    "        res = eng.RT_control_return(Tref, Iref, matlab.double([[Aref1],[Aref2],[Aref3]]), dA, stdout=io.StringIO())\n",
    "        return torch.from_numpy(np.asarray(res))\n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.gp_regression import FixedNoiseGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood, ExactMarginalLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "from botorch.acquisition.objective import GenericMCObjective, MCAcquisitionObjective\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import FastNondominatedPartitioning\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.acquisition.multi_objective.objective import MCMultiOutputObjective\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_RESTARTS = 256 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
    "\n",
    "bounds = torch.tensor([[35, 1500, 1.4016/2., -14.4395*2., 12.5570/2.],\n",
    "                       [65, 3000, 1.4016*2., -14.4395/2., 12.5570*2.]]).to(**tkwargs)\n",
    "#bounds = torch.tensor([[35, 1500, 1.4016*1.0, -14.4395*1.5, 12.5570*1.0],\n",
    "#                       [65, 3000, 1.4016*1.5, -14.4395*1.0, 12.5570*1.5]]).to(**tkwargs)\n",
    "standard_bounds = torch.zeros(2, bounds.shape[1], **tkwargs)\n",
    "standard_bounds[1] = 1\n",
    "# need to also include the bounds for the run idx\n",
    "# though this is fixed during AF optimization\n",
    "bounds = torch.cat((bounds, torch.tensor([[0],[100]]).to(**tkwargs)), dim=-1)\n",
    "bounds[1, -1] = 1.\n",
    "# we do not normalized the time (index)\n",
    "standard_bounds = torch.cat((standard_bounds, torch.tensor([[0],[100]]).to(**tkwargs)), dim=-1)\n",
    "# extra bounds for time (index) to make the AF optimization happy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "from botorch.models.transforms.input import InputTransform\n",
    "from gpytorch import Module as GPyTorchModule\n",
    "from botorch.models.transforms.utils import subset_transform\n",
    "from torch.distributions import MultivariateNormal\n",
    "from gpytorch.constraints import Interval\n",
    "import math\n",
    "\n",
    "class PosEncode(InputTransform, GPyTorchModule):\n",
    "    r\"\"\"A transform that uses learned input warping functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        positional_emb_dim: int,\n",
    "        reduce_to_one: bool = False,\n",
    "        transform_on_train: bool = True,\n",
    "        transform_on_eval: bool = True,\n",
    "        transform_on_fantasize: bool = True,\n",
    "        batch_shape: Optional[torch.Size] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"Initialize transform.\n",
    "\n",
    "        Args:\n",
    "            positional_emb_dim: Int, the positional embedding dimension, must be an even number.\n",
    "            reduce_to_one: Bool, if we project the positional embedding to one variable (using NN like layer).\n",
    "            transform_on_train: A boolean indicating whether to apply the\n",
    "                transforms in train() mode. Default: True.\n",
    "            transform_on_eval: A boolean indicating whether to apply the\n",
    "                transform in eval() mode. Default: True.\n",
    "            transform_on_fantasize: A boolean indicating whether to apply the\n",
    "                transform when called from within a `fantasize` call. Default: True.\n",
    "            batch_shape: The batch shape.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.positional_emb_dim = positional_emb_dim\n",
    "        self.reduce_to_one = reduce_to_one\n",
    "        self.transform_on_train = transform_on_train\n",
    "        self.transform_on_eval = transform_on_eval\n",
    "        self.transform_on_fantasize = transform_on_fantasize\n",
    "        self.batch_shape = batch_shape or torch.Size([])\n",
    "        if len(self.batch_shape) > 0:\n",
    "            # Note: this follows the gpytorch shape convention for lengthscales\n",
    "            # There is ongoing discussion about the extra `1`.\n",
    "            # TODO: update to follow new gpytorch convention resulting from\n",
    "            # https://github.com/cornellius-gp/gpytorch/issues/1317\n",
    "            batch_shape = self.batch_shape + torch.Size([1])\n",
    "        else:\n",
    "            batch_shape = self.batch_shape\n",
    "        if self.reduce_to_one:\n",
    "            # register weights\n",
    "            self.register_parameter('linear_W', torch.nn.Parameter(torch.zeros((positional_emb_dim, 1)).to(**tkwargs)))\n",
    "            torch.nn.init.kaiming_normal_(self.linear_W)\n",
    "            # register bias\n",
    "            self.register_parameter('linear_B', torch.nn.Parameter(torch.zeros((1,1)).to(**tkwargs)))\n",
    "            torch.nn.init.kaiming_normal_(self.linear_B)\n",
    "        self.register_parameter('positive_increase_scaler', torch.nn.Parameter(5*torch.ones((1,1)).to(**tkwargs)))\n",
    "        self.register_constraint('positive_increase_scaler', Interval(0, 10))\n",
    "\n",
    "    def get_positional_encoding(self, positional_emb_dim, idx_list):\n",
    "        position = idx_list.reshape(-1).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, positional_emb_dim, 2) * (-math.log(10000.0) / positional_emb_dim)).to(**tkwargs)\n",
    "        # Introduce a learnable scaling factor that increases with positional index\n",
    "        scaling_factor = torch.log(1+position*self.positive_increase_scaler)\n",
    "        pe = torch.zeros(len(position), 1, positional_emb_dim).to(**tkwargs)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term) * scaling_factor\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term) * scaling_factor\n",
    "        return pe.squeeze(1) # (len, dim)\n",
    "\n",
    "    @subset_transform\n",
    "    def transform(self, X: Tensor) -> Tensor:\n",
    "        r\"\"\"Warp the inputs.\n",
    "\n",
    "        Args:\n",
    "            X: A `input_batch_shape x (batch_shape) x n x d`-dim tensor of inputs.\n",
    "                batch_shape here can either be self.batch_shape or 1's such that\n",
    "                it is broadcastable with self.batch_shape if self.batch_shape is set.\n",
    "\n",
    "        Returns:\n",
    "            A `input_batch_shape x (batch_shape) x n x d`-dim tensor of transformed\n",
    "                inputs.\n",
    "        \"\"\"\n",
    "        positions = X.reshape(-1, X.shape[-1])[..., -1]\n",
    "        PEs = self.get_positional_encoding(self.positional_emb_dim, positions)\n",
    "        if self.reduce_to_one:\n",
    "            # maybe need to pull the distribution back?\n",
    "            # apply abs and a scaler factor, (almost) making sure the scalar is increasing\n",
    "            # as the index increases\n",
    "            PEs = PEs.abs()@torch.abs(self.linear_W)/math.sqrt(PEs.shape[-1]) + torch.abs(self.linear_B)\n",
    "            PEs = torch.nn.functional.gelu(PEs).reshape(X.shape[:-1]+(1, ))\n",
    "        else:\n",
    "            PEs = PEs.reshape(X.shape[:-1]+(self.positional_emb_dim,))\n",
    "        \n",
    "        return torch.cat((X[...,:-1], PEs), dim=-1)\n",
    "    \n",
    "    @property\n",
    "    def _k(self) -> MultivariateNormal:\n",
    "        \"\"\"Returns a MultivariateNormal distribution.\"\"\"\n",
    "        return MultivariateNormal(\n",
    "            loc=torch.zeros((self.positional_emb_dim+1)),\n",
    "            covariance_matrix=torch.eye((self.positional_emb_dim+1)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from covar_module import get_spatio_temp_kernel\n",
    "NOISE_SE = torch.tensor([0.0] * 1, **tkwargs)\n",
    "\n",
    "def generate_initial_data(problem, n=7, guaratee_feasible=True):\n",
    "    # generate training data\n",
    "    train_x = draw_sobol_samples(bounds=bounds[:,:-1], n=n, q=1).squeeze(1)\n",
    "    train_obj_plain = torch.cat([problem(*_.tolist()).reshape(1,-1) for _ in train_x], dim=0)\n",
    "    train_obj_true = train_obj_plain[..., :-1]\n",
    "    train_obj = train_obj_true + torch.randn_like(train_obj_true) * NOISE_SE\n",
    "    if guaratee_feasible:\n",
    "        final_train_x = torch.zeros((0, train_x.shape[-1]))\n",
    "        final_train_obj_true = torch.zeros((0, train_obj_true.shape[-1]))\n",
    "        final_train_obj = torch.zeros_like(final_train_obj_true)\n",
    "        mask = (train_obj_plain[:, -1] == 1).reshape(-1)\n",
    "        final_train_x = torch.cat((final_train_x, train_x[mask, :]), dim=0)\n",
    "        final_train_obj_true = torch.cat((final_train_obj_true, train_obj_true[mask, :]), dim=0)\n",
    "        final_train_obj = torch.cat((final_train_obj, train_obj[mask, :]), dim=0)\n",
    "        n = n-final_train_x.shape[0]\n",
    "        while n:\n",
    "            train_x = draw_sobol_samples(bounds=bounds[:,:-1], n=n, q=1).squeeze(1)\n",
    "            train_obj_plain = torch.cat([problem(*_.tolist()).reshape(1,-1) for _ in train_x], dim=0)\n",
    "            train_obj_true = train_obj_plain[..., :-1]\n",
    "            train_obj = train_obj_true + torch.randn_like(train_obj_true) * NOISE_SE\n",
    "            mask = (train_obj_plain[:, -1] == 1).reshape(-1)\n",
    "            final_train_x = torch.cat((final_train_x, train_x[mask, :]), dim=0)\n",
    "            final_train_obj_true = torch.cat((final_train_obj_true, train_obj_true[mask, :]), dim=0)\n",
    "            final_train_obj = torch.cat((final_train_obj, train_obj[mask, :]), dim=0)\n",
    "            n = n-train_x[mask, :].shape[0]\n",
    "        return final_train_x, final_train_obj, final_train_obj_true\n",
    "\n",
    "    return train_x, train_obj, train_obj_true\n",
    "\n",
    "\n",
    "def initialize_model(train_x, \n",
    "                     train_obj, \n",
    "                     input_trans=None, \n",
    "                     input_trans_args=None,\n",
    "                     type_of_forgetting='UI',\n",
    "                     forgetting_factor=0.03):\n",
    "    # define models for objective and constraint\n",
    "    train_x = normalize(train_x, bounds)\n",
    "    models = []\n",
    "    for i in range(train_obj.shape[-1]):\n",
    "        train_y = train_obj[..., i : i + 1]\n",
    "        train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
    "        covar = get_spatio_temp_kernel(train_x,\n",
    "                                       train_y, \n",
    "                                       type_of_forgetting=type_of_forgetting,\n",
    "                                       forgetting_factor=forgetting_factor)\n",
    "        if input_trans is not None:\n",
    "            models.append(FixedNoiseGP(train_x,\n",
    "                                       train_y, \n",
    "                                       train_yvar, \n",
    "                                       covar_module=covar,\n",
    "                                       outcome_transform=Standardize(m=1), \n",
    "                                       input_transform=input_trans(**input_trans_args)))\n",
    "        else:         \n",
    "            models.append(FixedNoiseGP(train_x, \n",
    "                                       train_y, \n",
    "                                       train_yvar,\n",
    "                                       covar_module=covar, \n",
    "                                       outcome_transform=Standardize(m=1)))\n",
    "    model = models[0] # in this case it is single objective!!!\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a MCMultiOutputObjective convert thickness and time to\n",
    "# (thickness-ref_thickness)**2 and (time-ref_time)**2\n",
    "\n",
    "class obj_convert(MCAcquisitionObjective):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, samples, X = None):\n",
    "        ans = -(samples-1.)**2\n",
    "        if ans.shape[-1] == 1:\n",
    "            ans = ans.squeeze(-1)\n",
    "        return ans # just the thickness\n",
    "\n",
    "\n",
    "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler, problem, run_num):\n",
    "    \"\"\"Optimizes the qNEI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    batch = BATCH_SIZE\n",
    "    # build the casadi optimization for MPC retry here\n",
    "    final_new_x = torch.zeros((0, train_x.shape[-1])).to(train_x)\n",
    "    final_new_obj = torch.zeros((0, train_obj.shape[-1])).to(train_obj)\n",
    "    final_new_obj_true = torch.zeros((0, train_obj.shape[-1])).to(train_obj)\n",
    "\n",
    "    # check how many recalculation we did....\n",
    "    retry_count = 0\n",
    "    while batch and retry_count <= BATCH_SIZE*100:\n",
    "        retry_count += batch\n",
    "        # partition non-dominated space into disjoint rectangles\n",
    "        acq_func = qNoisyExpectedImprovement(\n",
    "            model=model,\n",
    "            objective=obj_convert(),\n",
    "            X_baseline=normalize(train_x, bounds),\n",
    "            prune_baseline=True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "            sampler=sampler,\n",
    "        )\n",
    "        # optimize\n",
    "        candidates, _ = optimize_acqf(\n",
    "            acq_function=acq_func,\n",
    "            bounds=standard_bounds,\n",
    "            fixed_features={bounds.shape[-1]-1: float(run_num)},\n",
    "            q=batch,\n",
    "            num_restarts=NUM_RESTARTS,\n",
    "            raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "            options={\"batch_limit\": 5, \"maxiter\": 1000},\n",
    "            sequential=True,\n",
    "        )\n",
    "        # observe new values\n",
    "        new_x = unnormalize(candidates[...,:-1].detach(), bounds=bounds[:,:-1])\n",
    "        new_obj_true = torch.cat([problem(*_.tolist()).reshape(1,-1) for _ in new_x], dim=0)\n",
    "        #print(new_obj_true)\n",
    "        # check solution status\n",
    "        succeed = new_obj_true[:, -1] == 1.\n",
    "        #print(succeed)\n",
    "        new_x = new_x[succeed.reshape(-1), :]\n",
    "        #print(new_x.shape)\n",
    "        candidates = candidates[succeed.reshape(-1), :]\n",
    "        # chop donw the final dimension, which is just the MPC optimization succeed status\n",
    "        new_obj_true = new_obj_true[succeed.reshape(-1), :-1]\n",
    "        new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "        new_x = torch.cat((new_x, candidates[...,-1].detach().unsqueeze(-1)), dim=-1)\n",
    "\n",
    "        final_new_x = torch.cat([final_new_x, new_x], dim=0)\n",
    "        final_new_obj = torch.cat([final_new_obj, new_obj], dim=0)\n",
    "        final_new_obj_true = torch.cat([final_new_obj_true, new_obj_true], dim=0)\n",
    "\n",
    "        # check if we need more calculation...\n",
    "        batch = batch - new_x.shape[0]\n",
    "\n",
    "    return final_new_x, final_new_obj, final_new_obj_true, retry_count <= BATCH_SIZE*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open('../data_init.pkl', 'rb') as fp:\n",
    "    old_res = pickle.load(fp)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_init(n=5):\n",
    "    cur_problem = problem_wrapper(1.0)\n",
    "    #train_init = generate_initial_data(cur_problem, n=n)\n",
    "    train_init = (old_res['outcome_X'][:10,:-1], old_res['outcome_Y'][:10,:], old_res['outcome_Y'][:10,:])\n",
    "    def BO_wrapper_with_same_init(embed_dim,\n",
    "                                  reduce_to_one, \n",
    "                                  batch_num,\n",
    "                                  type_of_forgetting='UI',\n",
    "                                  forgetting_factor=0.03):\n",
    "        import time\n",
    "        import warnings\n",
    "\n",
    "        from botorch.exceptions import BadInitialCandidatesWarning\n",
    "        from botorch.fit import fit_gpytorch_model\n",
    "        from botorch.sampling import SobolQMCNormalSampler\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        N_BATCH = batch_num if not SMOKE_TEST else 10        \n",
    "        MC_SAMPLES = 512 if not SMOKE_TEST else 16\n",
    "\n",
    "        verbose = True\n",
    "\n",
    "        while True:\n",
    "            restart_flag = False\n",
    "            # call helper functions to generate initial training data and initialize model\n",
    "            res = {}\n",
    "            train_x_qnehvi, train_obj_qnehvi, train_obj_true_qnehvi = train_init\n",
    "            # need to assign index of 0, indicating we have data of the un-drifted model\n",
    "            train_x_qnehvi = torch.cat((train_x_qnehvi, torch.zeros((train_x_qnehvi.shape[0], 1)).to(**tkwargs)), dim=-1)\n",
    "            if embed_dim is not None:\n",
    "                mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi, \n",
    "                                                            PosEncode, {\"positional_emb_dim\": embed_dim, \"reduce_to_one\": reduce_to_one},\n",
    "                                                            type_of_forgetting=type_of_forgetting, forgetting_factor=forgetting_factor)\n",
    "            else:\n",
    "                mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi,\n",
    "                                                            type_of_forgetting=type_of_forgetting, forgetting_factor=forgetting_factor)\n",
    "\n",
    "            # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "            for iteration in range(2, N_BATCH + 1):\n",
    "\n",
    "                t0 = time.monotonic()\n",
    "\n",
    "                # fit the models\n",
    "                #for name, _ in mll_qnehvi.named_parameters():\n",
    "                #    print(name, _)\n",
    "                fit_gpytorch_model(mll_qnehvi)\n",
    "                #for name, _ in mll_qnehvi.named_parameters():\n",
    "                #    print(name, _)\n",
    "\n",
    "                # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "                qnehvi_sampler = SobolQMCNormalSampler(MC_SAMPLES)\n",
    "                #if iteration > 20:\n",
    "                #    run_num = 20 - 1\n",
    "                #else:\n",
    "                #    run_num = iteration-1\n",
    "                #dA_init = 1.0\n",
    "                #dA_final = 1.5\n",
    "                #No_run_max = 20\n",
    "                #dA = dA_init - (dA_init - dA_final)/(No_run_max-1)*(run_num)\n",
    "                \n",
    "                run_num = iteration-1\n",
    "                #dA = 0.5 /(1 + math.exp(-0.15*(run_num-50-1)))+1            \n",
    "                dA = 0.5 /(1 + math.exp(-0.10*(run_num-50-1)))+1            \n",
    "                print('\\n', dA)\n",
    "                cur_problem = problem_wrapper(dA)\n",
    "\n",
    "                # optimize acquisition functions and get new observations\n",
    "                (\n",
    "                    new_x_qnehvi,\n",
    "                    new_obj_qnehvi,\n",
    "                    new_obj_true_qnehvi,\n",
    "                    exit_status,\n",
    "                ) = optimize_qnehvi_and_get_observation(model_qnehvi, train_x_qnehvi, train_obj_qnehvi, qnehvi_sampler, cur_problem, run_num)\n",
    "\n",
    "                # if we failed to optimize several times (currently BATCH_SIZE*100), we just dump this trajectory!\n",
    "                if not exit_status and new_x_qnehvi.shape[0] != BATCH_SIZE:\n",
    "                    restart_flag = True\n",
    "                    continue\n",
    "                # update training points\n",
    "                train_x_qnehvi = torch.cat([train_x_qnehvi, new_x_qnehvi])\n",
    "                train_obj_qnehvi = torch.cat([train_obj_qnehvi, new_obj_qnehvi])\n",
    "                train_obj_true_qnehvi = torch.cat([train_obj_true_qnehvi, new_obj_true_qnehvi])\n",
    "\n",
    "                # reinitialize the models so they are ready for fitting on next iteration\n",
    "                # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "                # using the hyperparameters from the previous iteration\n",
    "                if embed_dim is not None:\n",
    "                    mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi, \n",
    "                                                                PosEncode, {\"positional_emb_dim\": embed_dim, \"reduce_to_one\": reduce_to_one},\n",
    "                                                                type_of_forgetting=type_of_forgetting, forgetting_factor=forgetting_factor)\n",
    "                else:\n",
    "                    mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi,\n",
    "                                                                type_of_forgetting=type_of_forgetting, forgetting_factor=forgetting_factor)\n",
    "\n",
    "                t1 = time.monotonic()\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"\\nBatch {iteration:>2}: \"\n",
    "                        f\"time = {t1-t0:>4.2f}\",\n",
    "                        end=\"\",\n",
    "                    )\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "            if restart_flag:\n",
    "                continue\n",
    "            res[\"outcome_X\"] = train_x_qnehvi\n",
    "            res[\"outcome_Y\"] = train_obj_qnehvi\n",
    "            return res\n",
    "    return BO_wrapper_with_same_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_drift = same_init(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plain_res = []\n",
    "#for _ in range(1):\n",
    "    # plain_res.append(run_drift(None, False, 30, 'UI', 0.03))\n",
    "    # plain_res.append(run_drift(None, False, 100, 'UI', 0.03))\n",
    "#    plain_res.append(run_drift(None, False, 0, 'UI_learning', 0.03))\n",
    "\n",
    "pos_emb_res = []\n",
    "for _ in range(5):\n",
    "    # pos_emb_res.append(run_drift(8, True, 30, 'UI', 0.03))\n",
    "    # pos_emb_res.append(run_drift(128, True, 30, 'UI', 0.03))\n",
    "    # pos_emb_res.append(run_drift(64, True, 30, 'UI', 0.03))\n",
    "    pos_emb_res.append(run_drift(64, True, 100, 'UI_learning', 0.03))\n",
    "    # pos_emb_res.append(run_drift(None, False, 100, 'vanilla_with_index', 0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first argument is the dimension of positional encoding, must be even\n",
    "# if None, then the index of the run will be used\n",
    "# the indices of all the initial data set are set as 0 (perfect environment)\n",
    "# the second is if we map the positional encoding dimension to 1\n",
    "# the third is the total number of runs, excluding the first 5 init runs.\n",
    "# the fourth is the method of TVBO (the temporal kernel)\n",
    "# the fifth is the parameter of the temporal kernel, the original paper used 0.03\n",
    "res = run_drift(None, False, 30, 'UI', 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('single_UI_withoutdB.pkl','wb') as fp:\n",
    "    pickle.dump(plain_res, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plain_res_array = torch.tensor([plain_res[_]['outcome_Y'].tolist() for _ in range(5)]).squeeze(-1)\n",
    "pos_emb_res_array = torch.tensor([pos_emb_res[_]['outcome_Y'].tolist() for _ in range(5)]).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "dA_init = 1.0\n",
    "dA_final = 1.5\n",
    "No_run_max = 20\n",
    "w_change = [1.,1.,1.,1.,1.]*2\n",
    "#for _ in range(2, 31):\n",
    "#    if _ > 20:\n",
    "#        _ = 20\n",
    "#    dA = dA_init - (dA_init - dA_final)/(No_run_max-1)*(_-1)\n",
    "#    w_change.append(dA)\n",
    "\n",
    "for _ in range(2, 101):\n",
    "    #if _ > 20:\n",
    "    #    _ = 20\n",
    "    #dA = dA_init - (dA_init - dA_final)/(No_run_max-1)*(_-1)\n",
    "    dA = 0.5 /(1 + math.exp(-0.15*(_-50-1)))+1\n",
    "    w_change.append(dA)\n",
    "\n",
    "\n",
    "axs.plot(range(1, 110), plain_res_array.mean(0))\n",
    "axs.fill_between(range(1, 110), \n",
    "                 plain_res_array.mean(0)-plain_res_array.std(0), \n",
    "                 plain_res_array.mean(0)+plain_res_array.std(0),\n",
    "                 alpha=0.2)\n",
    "axs.plot(range(1, 110), [1.]*109)\n",
    "axs.plot(range(1, 110), w_change, 'g--')\n",
    "#axs.set_ylabel('Thickness (mm)')\n",
    "axs.set_ylim([0,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "#dA_init = 1.0\n",
    "#dA_final = 1.5\n",
    "#No_run_max = 20\n",
    "w_change = [1.,1.,1.,1.,1.]*2\n",
    "for _ in range(2, 101):\n",
    "    #if _ > 20:\n",
    "    #    _ = 20\n",
    "    #dA = dA_init - (dA_init - dA_final)/(No_run_max-1)*(_-1)\n",
    "    #dA = 0.5 /(1 + math.exp(-0.15*(_-50-1)))+1\n",
    "    dA = 0.5 /(1 + math.exp(-0.10*(_-50-1)))+1\n",
    "    w_change.append(dA)\n",
    "    \n",
    "axs.plot(range(1, 110), pos_emb_res_array.mean(0))\n",
    "axs.fill_between(range(1, 110), \n",
    "                 pos_emb_res_array.mean(0)-pos_emb_res_array.std(0), \n",
    "                 pos_emb_res_array.mean(0)+pos_emb_res_array.std(0),\n",
    "                 alpha=0.2)\n",
    "axs.plot(range(1, 110), [1.]*109)\n",
    "axs.plot(range(1, 110), w_change, 'g--')\n",
    "axs.set_ylabel('Thickness (mm)')\n",
    "axs.set_ylim([0,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "#dA_init = 1.0\n",
    "#dA_final = 1.5\n",
    "#No_run_max = 20\n",
    "w_change = [1.,1.,1.,1.,1.]*2\n",
    "for _ in range(2, 101):\n",
    "    #if _ > 20:\n",
    "    #    _ = 20\n",
    "    #dA = dA_init - (dA_init - dA_final)/(No_run_max-1)*(_-1)\n",
    "    dA = 0.5 /(1 + math.exp(-0.15*(_-50-1)))+1\n",
    "    if 20 < _ < 80:\n",
    "        dA = dA + math.sin((_-20)/130)/20\n",
    "    w_change.append(dA)\n",
    "    \n",
    "#axs.plot(range(1, 110), pos_emb_res_array.mean(0))\n",
    "#axs.fill_between(range(1, 110), \n",
    "#                 pos_emb_res_array.mean(0)-pos_emb_res_array.std(0), \n",
    "#                 pos_emb_res_array.mean(0)+pos_emb_res_array.std(0),\n",
    "#                 alpha=0.2)\n",
    "axs.plot(range(1, 110), [1.]*109)\n",
    "axs.plot(range(1, 110), w_change, 'g--')\n",
    "axs.set_ylabel('Thickness (mm)')\n",
    "axs.set_ylim([0,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indices and the modified function with trigonometric functions added to create local minima and maxima\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ori_indeices = np.array(range(-10, 81))\n",
    "indices =ori_indeices*1.4\n",
    "line = 1 + (0.5 / (1 + np.exp(-0.1 * (indices - 30)))) + 0.05 * np.sin(0.2 * indices) + 0.03 * np.cos(0.5 * indices)\n",
    "line = [1.,1.,1.,1.,1.]*2 + line.tolist()+[1.5] * 10\n",
    "line[10] = 1.005\n",
    "line[11] = 1.01\n",
    "line[12] = 1.016\n",
    "line[13] = 1.022\n",
    "line[14] = 1.030\n",
    "line[15] = 1.037\n",
    "line[16] = 1.043\n",
    "line[17] = 1.051\n",
    "line[18] = 1.053\n",
    "line[19] = 1.057\n",
    "line[20] = 1.060\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(line)), line, label='Smooth Asymptotic Line with Trigonometric Modulations', color='blue')\n",
    "\n",
    "# Setting the limit for y-axis to make the asymptotic nature clearer\n",
    "plt.ylim(0.9, 1.6)\n",
    "\n",
    "# Annotate the asymptotic values\n",
    "#plt.annotate('Asymptotic value = 1', xy=(0, 1), xytext=(5, 1.1),\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "#plt.annotate('Asymptotic value = 1.5', xy=(60, 1.5), xytext=(45, 1.55),\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "# Set the range for the x-axis\n",
    "#plt.xlim(0, 81)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Add a title and a legend\n",
    "plt.title('Smooth Asymptotic Line from Index 0 to 60 with Local Minima and Maxima')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "wrong_res = pickle.load(open('all_run_correct.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.utils.multi_objective.box_decompositions.dominated import DominatedPartitioning\n",
    "bd = DominatedPartitioning(ref_point=torch.tensor([-5., -5.]), Y=obj_convert()(ans[\"outcome_Y\"]))\n",
    "volume = bd.compute_hypervolume().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb_res_array_X = torch.tensor([pos_emb_res[_]['outcome_X'].tolist() for _ in range(5)]).squeeze(-1)\n",
    "pos_emb_res_array_X_np = pos_emb_res_array_X.numpy()\n",
    "\n",
    "pos_emb_res_array_Y = torch.tensor([pos_emb_res[_]['outcome_Y'].tolist() for _ in range(5)]).squeeze(-1)\n",
    "pos_emb_res_array_Y_np = pos_emb_res_array_Y.numpy()\n",
    "\n",
    "XY_Run0 = np.array([[50.0, 2000.0, 1.4016, -14.4395, 12.5570, 0.0, 1.0048, 1.0]])\n",
    "\n",
    "k = 0\n",
    "X_temp = np.array(pos_emb_res_array_X_np[k])\n",
    "Y_temp = np.array(pos_emb_res_array_Y_np[k])\n",
    "XY_Run_temp = np.vstack((X_temp.T,Y_temp)).T\n",
    "XY_Run = np.vstack((XY_Run_temp.T,w_change)).T\n",
    "XY_Run = np.vstack((XY_Run0,XY_Run))\n",
    "data_to_save = {'STVBO_wPOS_1': XY_Run}\n",
    "savemat('STVBO_wPOS_1.mat', data_to_save)\n",
    "#XY_Run1_matlab = '[' + '; '.join([' '.join(map(str, row)) for row in XY_Run]) + ']'\n",
    "\n",
    "k = 1\n",
    "X_temp = np.array(pos_emb_res_array_X_np[k])\n",
    "Y_temp = np.array(pos_emb_res_array_Y_np[k])\n",
    "XY_Run_temp = np.vstack((X_temp.T,Y_temp)).T\n",
    "XY_Run = np.vstack((XY_Run_temp.T,w_change)).T\n",
    "XY_Run = np.vstack((XY_Run0,XY_Run))\n",
    "data_to_save = {'STVBO_wPOS_2': XY_Run}\n",
    "savemat('STVBO_wPOS_2.mat', data_to_save)\n",
    "#XY_Run2_matlab = '[' + '; '.join([' '.join(map(str, row)) for row in XY_Run]) + ']'\n",
    "\n",
    "k = 2\n",
    "X_temp = np.array(pos_emb_res_array_X_np[k])\n",
    "Y_temp = np.array(pos_emb_res_array_Y_np[k])\n",
    "XY_Run_temp = np.vstack((X_temp.T,Y_temp)).T\n",
    "XY_Run = np.vstack((XY_Run_temp.T,w_change)).T\n",
    "XY_Run = np.vstack((XY_Run0,XY_Run))\n",
    "data_to_save = {'STVBO_wPOS_3': XY_Run}\n",
    "savemat('STVBO_wPOS_3.mat', data_to_save)\n",
    "#XY_Run3_matlab = '[#' + '; '.join([' '.join(map(str, row)) for row in XY_Run]) + ']'\n",
    "\n",
    "k = 3\n",
    "X_temp = np.array(pos_emb_res_array_X_np[k])\n",
    "Y_temp = np.array(pos_emb_res_array_Y_np[k])\n",
    "XY_Run_temp = np.vstack((X_temp.T,Y_temp)).T\n",
    "XY_Run = np.vstack((XY_Run_temp.T,w_change)).T\n",
    "XY_Run = np.vstack((XY_Run0,XY_Run))\n",
    "data_to_save = {'STVBO_wPOS_4': XY_Run}\n",
    "savemat('STVBO_wPOS_4.mat', data_to_save)\n",
    "#XY_Run4_matlab = '[' + '; '.join([' '.join(map(str, row)) for row in XY_Run]) + ']'\n",
    "\n",
    "k = 4\n",
    "X_temp = np.array(pos_emb_res_array_X_np[k])\n",
    "Y_temp = np.array(pos_emb_res_array_Y_np[k])\n",
    "XY_Run_temp = np.vstack((X_temp.T,Y_temp)).T\n",
    "XY_Run = np.vstack((XY_Run_temp.T,w_change)).T\n",
    "XY_Run = np.vstack((XY_Run0,XY_Run))\n",
    "data_to_save = {'STVBO_wPOS_5': XY_Run}\n",
    "savemat('STVBO_wPOS_5.mat', data_to_save)\n",
    "#XY_Run5_matlab = '[' + '; '.join([' '.join(map(str, row)) for row in XY_Run]) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
